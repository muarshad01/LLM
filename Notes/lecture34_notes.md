
* [Datasets & DataLoaders](https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html)
* (Input, Target) pairs

***


5:34
the reason we are using this data set and data loaders is ultimately we want to take our data set and convert it into
5:40
input Target batches like these so this is just one batch we'll have multiple batches since we have 747 samples so
5:48
this is a batch of only eight samples so this is where we want to reach at the end of the lecture uh but
Equal text length for all data samples
5:55
at the start of the lecture we are at this point where we have uh text
6:00
um and here I have provided a snippet of what each email looks like and we have labels as zero or one this is where we
6:07
are right now but here if you see we want every text email to be of the same
6:13
token length right however if you look at the email length the email length is
6:18
not the same one email might be longer one email might be shorter how do we make sure that all the email or all the
6:26
text messages rather are of the same length so the first problem is that in our data set the text messages are of
6:32
varying length but we want to create a batch like this right and when we are dealing with batches every every row
6:39
here should have the same number of columns so we need to somehow make sure that every text message has the same
6:45
length and there are two options to do this first what we do is that you check
6:50
all the emails and then you find that email which has the shortest length so let's say let's say there are five
6:57
emails right now and who lengths I've given to be representative by these
7:03
sizes there are five emails the first option is that you just look at the shortest length which is this and you
7:08
truncate all other emails to this and you get rid of the remaining
7:14
part in all these other images so then the size of all the emails will be the same or the text messages and then we
7:20
can group them in a batch can you think of what the disadvantage would be of this particular
7:27
approach the disadvantage would be that that will lose all of this information which is present otherwise in the longer
7:34
text so if the data set consists of emails which are much longer than this
7:40
shorter shortest email then we'll lose all of that data so this is not a recommended approach so what can we do
7:47
as an alternative approach well the alternative approach is as follows what if we use the longest so what if we use
7:55
the first let me rub this so that um all of us are on the same page okay so I've
8:01
rubb this now so what if we use the longest email uh let's say this is the
8:08
longest email and then for all the other emails we pad them with certain tokens
8:14
we pad them which means we add additional tokens till they reach the longest
8:19
email so this is what we are actually going to do because this will make sure that we don't lose out on any
8:25
information so your question would be what are we going to pad them with we are going to pad them with a token which
End of text padding
8:31
is called as the end of text token I'll talk about this in a moment
8:39
uh but first let me illustrate what does it mean by we are going to pad all the
8:44
messages to match the length of the longest message so the way this looks like is that let's say we have a first
8:51
text message we tokenize it first right and the way to
8:56
tokenize Any Given sentence is basically we are going to use the bite pair encoder so if you have followed the
9:03
previous lectures there is this Library called tick token and tick token is a tokenizer library which open AI uses
9:10
what this tick token does is that you give it any sentence it converts it into a bunch of token IDs now it's a bite
9:18
pair encoder which means every word is not equal to one token let's say if you have given the word hello word this will
9:24
not just be two tokens simply or three tokens because of the space bar bite pair encoder is a subword tokenizer so
9:32
it's a bit more complex than that and uh based on this tokenizer um every
9:38
sentence will be converted into a bunch of tokens so the first step is that we are going to take input messages and we
9:45
are going to tokenize them uh using the bite pair encoder tokenizer which is the
9:50
one which also GPT models use so every text is going to be converted into token IDs like this now of course some text
9:58
would be longer some would be shorter so the number of token IDs won't match what we are then going to do is that let's
10:04
say I have only three text messages in the data set this looks to be the longest right so what I'll do is that
10:11
for the other ones I'm going to pad this with a token called f token named 50256
10:18
until uh the length of all the token IDs is the same now what is this



***


10:24
50256 let me uh show you what this 50256 is so here's what I'm showing you on the
10:31
screen right now is the vocabulary which is used by gpt2 what is meant by vocabulary is that there are tokens and
10:37
every token has an Associated token ID right so now if you scroll down to the
10:42
end of this you'll see that the vocabulary size which gpt2 uses is
10:48
essentially uh 50257 so it starts with 0 the token
10:54
ID and if you look at the last entry in this vocabulary it's this end of text and and the token ID corresponding to
11:00
this end of text is 50256 this signifies that we have reached the end of a document and or end
11:08
of a sentence and then we are starting the next sentence so typically this end of text token is used by while training
11:15
GPT to distinguish between separate document sources and what this vocabulary means is that essentially if
11:21
you are given any text right um it's first the text is essentially converted
11:27
into a bunch of tokens and then those tokens are assigned token IDs based on this vocabulary or based on
11:34
this dictionary so it's kind of the you can think of it like a dictionary Oxford
11:40
dictionary right you have words and you have their meanings similarly here you have words and there is a token ID
11:46
associated with each word so remember this 50256 that's essentially the end of text what we are going to do here is
11:53
that all the text messages which have a lesser number of token IDs we are going
11:58
to append them with this end of text as the padding token 50256 ideally we can use any token as
12:05
the padding token but the thing is the end of text just is makes symbolic sense
12:10
right because it signifies that there are no additional letters over here so
12:16
when the model will encounter end of text it will not get confused by any random word so that way that's why we
12:23
have used 50256 okay so overall the workflow which
12:29
we are going to follow is going to look like this we have the we have the input text which is the CSV files in the code
12:36
right now we have the train validation and test input text we are going to use a tokenizer and we are going to convert
12:43
this input text into a bunch of token IDs then what we are going to do is that we are going to look for that sentence
12:49
or that email in our data set which is the longest email and then we are going to make sure that all the text messages
12:55
have that same length by padding them with this uh end of text token of
13:01
50256 now let me take you through code and show you how this is exactly
Coding the Spam Dataset class in Python
13:07
implemented okay so uh
13:13
yeah so as we have seen earlier we first need to implement a pytorch data set
13:18
which specifies exactly how the data is loaded and processed before we can instantiate the data loader so there are
13:25
two things here there is a data set and data loader data data set specifies the
13:30
how how the data is supposed to be loaded what are the input and Target Pairs and data loader essentially then
13:36
we can instantiate the data loader but first the data set needs to be defined so that's why initially what we are
13:42
going to do is we are going to define the spam data set class what this class is going to do is that first of all it's
13:49
going to identify the longest sequence in the training data set why do we need the longest sequence because we are
13:54
going to make sure that all the sequences are of the same length as the longest sequence second what it does is
14:00
that it takes every text message and then it converts it into token IDs and finally the most important thing it does
14:07
is that it ensures that all the other sequences are padded with a padding token to match the length of the longest
14:13
sequence essentially this spam data set class is going to perform all of the three steps which we described over
14:20
here so let's get right into it and start understanding the class when you create an instance of the spam data set
14:27
class you will need to specify some things so first you have to specify the CSV file so imagine that we have given
14:33
the training data CSV file right and uh second you have to specify what is the
14:39
tokenizer which you're using we are going to use this tick token Library a bite pair encoder tokenizer to convert
14:47
the um to convert essentially the sentence into a bunch of tokens and then those tokens will be converted into
14:54
token IDs right then we have to specify the max ma length so this max length is
15:00
basically if you look at the output batch which we saw the max length will
15:06
be the maximum um length of the email in the entire data set so we are going to
15:13
look at that email which is the longest length but here what we have given is that we have even given a provision for
15:18
the user to externally Define the maximum length that is also possible um now the pad token ID is
15:25
50256 this means that all the emails which have length shorter than the maximum length we are going to pad them
15:32
with 50256 so the first step as we have seen on the Whiteboard over here first step
15:37
is this tokenization right so what we are going to do is that we are going to take the tokenizer and this will be the
15:44
bite pair encoder tokenizer uh we are going to take the text which is the text file
15:52
um and then what we are going to do is that we are going to take every single sentence and then we are going to convert that sentence into a bunch of


***



15:59
token IDs so tokenizer encod takes sentences converts it into tokens and
16:04
then using the vocabulary converts those tokens into token IDs so this data is
16:09
pd. read CSV so let's say we have passed this CSV file first we'll store the data
16:16
of the P of the CSV file into this data object and then what we'll do is that we'll look at every single email or text
16:23
message in this data and convert it into a bunch of token IDs to get a sense of the visual representation of this take a
16:30
look here so at this first step what we are doing is that we taking every single
16:35
text message and converting it into a bunch of token IDs right now if the user
16:40
has not specified a maximum length here what we'll first do is we'll find the maximum length of the text message in
16:47
the entire data set and that will be found through this longest encoded length token longest encoded length
16:53
function and if you scroll down below you'll see the longest encoded length what it does is that it just finds the
17:00
length of all of the text messages and then it Returns the maximum length uh
17:07
among all the text messages so now max length variable contains the longest
17:12
email length right this is if the user does not specify the max length if the
17:17
user has specified the max length then the max length will be equal to whatever the user has specified awesome now if
17:25
there are some sequences in the data set which are longer than the max maximum length this might happen if the user has
17:31
specified the max length and there are some sequences which are longer than the maximum length we'll have to truncate
17:37
those text messages so that their length equals the maximum length if the maximum
17:43
length is selected from the data itself this problem will not arise uh right now this is the next part
17:51
which is the most important what we do is that for all the text messages we are going to
17:59
append the token IDs this pad token ID which is 50256 we are going to append it
18:05
and how many such token idies we are going to append we are going to append how many our token IDs which are needed
18:11
to get that text message to the maximum length so that is essentially shown in
18:17
this step we are going to append the token IDs append the 50257 token ID to
18:22
all the text messages so that all of them have the same size and what is that size that size is max length
18:31
great and then finally this function is the most important what this function will do is that it will uh create two
18:38
such tensors it will create a tensor name encoded and it will create a tensor named label the tensor named encoded
18:46
basically uh will will make sure that every text message has
18:51
uh has this kind of a encoding in terms of token IDs like this what I've shown
18:56
on the screen right now so let me just re name this a bit so this Matrix which you seeing on the screen right
19:03
now and let me Mark it this this Matrix here that is the encoding Matrix or the
19:10
encoded uh let me check the name here the name is encoded so this is the encoded Matrix or tensor I should call
19:17
it and this Matrix which I'm or this tensor which I'm highlighting right now that's the label so what this data set
19:24
will do is that the main function it will is the get item function and when you you call get item it will convert
19:30
the data set into two tensors the encoded tensor and the labeled tensor the encoded will make sure that every
19:36
sentence in the data set is converted into a bunch of token IDs and all of the sentences have equal length so that they
19:43
can be batched together and then the label s tensor will just have zeros or
19:48
ones right so this is essentially uh what we are doing in the spam data set
19:55
class okay now uh I have just written here what the spam data set class does
20:01
the spam data set class loads the data from the CSV files which we have created earlier tokenizes the text using the



***


20:09
gpt2 tokenizer from tick token and then allows us to pad or truncate the
20:14
sequences to uniform length defined by either the longest sequence or predefined maximum length if the user
20:21
defines their own maximum length what we can now do is we can create an instance of the spam data set class using the
20:27
train. CSV file uh which we obtained in the previous lecture and here you can see I do not
20:34
set the maximum length so the maximum length is computed from the data set itself and when you print the maximum
20:39
length you can see that it's 120 that makes sense uh since the longest sequence in our data set contains no
20:46
more than 120 tokens it seems like a common length for text messages so one
20:51
sentence is around 15 to 20 tokens let's say so six sentences uh then that's the maximum
20:57
length so it's worth noting that the model can handle sequences of up to 1024
21:02
tokens because the context length of gpt2 which we have defined as the model here is
21:08
1024 right so you can pass max length Max up to a maximum value of 1024 over
21:15
here when you call this function um now what we are going to do
21:20
is that we are also going to uh pad the validation and test data sets to match the length of the longest training
21:27
sequence it it is important to note that any validation and test samples exceeding the length of the longest
21:33
training examples are truncated so now what we are going to do is that when we create an instance of the spam data set
21:39
class for the validation and the test data set we pass in the maximum length and that maximum length will be equal to
21:46
120 which is the maximum length in the training data set so we will encounter
21:51
this Loop where maximum length has been defined so there might be some sequences
21:56
in the tra testing and validation set which are last lger than the maximum length so we will need to truncate those
22:01
sequences that's what I've written over here however one thing to not is that
22:07
you can even set the maximum length equal to none here there is no such requirement that the maximum length
22:12
which you set here has to be equal to the maximum length in the training data set uh you can try out by setting this
22:19
to none as well okay now uh so here you can see
22:24
that you can create instances of the validation data set and the testing data set as well and then you can print out
22:31
the maximum length but right now if you print out the maximum length it will just be equal to 120 because we have
22:37
passed in that so if you print out the maximum length you can see that it's 120
22:43
because we passed in this parameter as a user defined and this was already
22:49
120 all right now the data set has been defined right now the data set will
Coding the Data Loaders
22:55
serve as an input to the data loader so so remember there are two things here first uh we have to implement a data set
23:03
and the data set will then be served as an input to the data loader so now what
23:08
we are going to do next is that we are going to use the data set as the input and then we will instantiate data
23:14
loaders right uh okay so in when we create or when we pass the data set as
23:21
an input to the data loader remember that we can set the batch size and we can also set the number of workers
23:27
that's for parallel processing so here what we are doing is that we are setting the batch size equal to 8 and we
23:32
are setting the number of workers equal to zero setting number of workers equal to zero is just for the sake of
23:38
Simplicity we don't want any parallel processing here drop last equal to True
23:43
means that if the last batch has a smaller data we just drop it uh great so
23:51
now here you can see train loader you create an instance of the data loader and then you pass in the train data set
23:58
as the input you similarly you can create the validation loader and the test loader as well so now this test
24:05
loader validation loader and trade loader when you run this part they'll be initialized and then you can use these
24:11
loaders to essentially create entire data sets in this format what I'm showing to you on the screen right now
24:17
so for example when you run the training data loader and you can extract batches from it now you can extract the first
24:24
batch and then it will give you the uh encoded and the you can extract the
24:29
second batch you can extract the eighth batch in a very easy manner so after you run this what we can now do is that we
24:36
can run a test to make sure that the data loaders are working and indeed returning batches of the expected size
24:43
so here what I'm doing is that I'm iterating through the training data loader uh right till the end and then
24:49
then I'm going to print the input badge Dimension and the label batch Dimension so if you uh iterate through the
24:56
training loader till the end you'll see that the input badge Dimension has the size of 8 by 120 can you think what this
25:02
means and the label badge Dimensions has torch dot size 8 can you try to think what this means you can pause the video
25:09
here for a moment so these input B Dimension means that since the batch size was eight
25:16
every input batch has eight rows and 120 columns because the maximum token ID
25:22
length was 120 so this is exactly what I've shown you over here right on the screen what you're seeing right now
25:29
uh on the screen what you're seeing is if you look at yeah if you look at this first answer
25:38
this right here what I'm showing with the arrow right now that that's the input badge and if you look at this input
25:45
batch you'll see that it has eight rows and it has 120 columns so that's why the size here is 8 by
25:52
120 okay similarly you can look at the labels label sensor and here you can see


***


25:59
that it just has eight rows over here and it can either have zeros or ones so that's why the size here is 8 and this
26:07
what I'm showing here is just one batch remember that there are 747 uh examples corresponding to spam
26:16
and 747 examples corresponding to no spam so if you add these both together
26:23
you'll get that the total number of data which we have is 1494 right out of that
26:28
the training data is 70% right so we will have batches corresponding to that so 70% of 1494
26:36
let's see so 7 into
26:41
1494 uh that's 1045 and I think we have printed this above
26:47
uh yeah 1045 so the training data set overall has 1045 samples right so the
26:55
training data set overall has uh training data set overall has
27:01
1045 samples in the training data set and now if each batch has eight such samples
27:10
the batch size is eight right which means each batch will have eight samples if each batch has eight samples eight
27:17
samples in each batch how many batches do you think will be in the training data
27:24
set so then the number of batches will be
27:30
number of batches will be 1045 divided by 8 which is approximately 130
27:36
batches and we can actually test this what I've done at the end of this code is that I have printed the length of the
27:42
training loader which will give me the number of training batches and here we can see that we have 130 training batches and that exactly fits our
27:49
intuition which you have written on the Whiteboard similarly what you can do is that you can print out the length of the
27:56
validation loader the length of the test loader as well and you'll see that there are 19 validation batches and 38 test
28:02
batches remember that we have 20% of the data as test and 10% of the data as
28:08
validation that's why the number of test batches are exactly two times that of the number of validation
28:14
batches and why does the length of the training loader give you the number of batches because if you print out one
28:21
batch that has eight eight samples right so actually the training loader this is
28:27
just one batch so the the training loader Dimensions will be so one batch of the training loader
28:33
has the dimension of uh 88 rows and 120 column right and there are 130 such
28:39
batches so if you print out the dimension of the entire training loader it will be 130 by 8X 120 it will be a
28:46
threedimensional tensor similarly for the testing loader
28:51
the dimensions would be uh 38 38 8 120
28:58
and for the validation the dimensions would be 19820 the dimensions of the validation
29:05
loader right so now until this part this concludes the data preparation steps
29:11
which means that now we have got these data loaders we have got the training loader validation loader and test loader
29:16
through these data loaders we can easily extract the batches the input batch and
29:21
the label batch which we need at any time and that just better so now we have created input and Target pairs remember
29:29
when we trained the llm the target pair here was also a token ID
29:34
representation it was just shifted from the input to the right by one so the difference here is that the target pair
29:41
here are zero and ones they are not token IDs awesome so until now we have looked
Recap and next steps
29:47
at this entire First Column is now over in the previous lecture we saw downloading the data set and
29:53
pre-processing the data set in today's lecture we saw how to create data loaders for the training data set the
29:59
validation data set and the testing data set in the next lectures we are going to start looking at the stage two which is
30:06
we are going to initialize the llm model we are going to load the pre-train weights we are going to modify the model
30:11
for fine tuning and then finally in the subsequent lectures we'll evaluate the F tune model so we have done the hard work
30:19
of data pre-processing data cleaning and using data sets and data loaders which is usually so important before we
30:26
directly jump to the model training itself so I hope you all are liking these
30:31
lectures I usually try to maintain an approach of whiteboard plus coding since
30:37
my aim is to train ml Engineers who are not just good at using chat GPD uh but I
30:43
want every one of you who are following these lectures to have very strong fundamental and theoretical understanding because that's what's
30:50
lacking in today's Engineers thanks a lot everyone and I look forward to seeing you in the next
30:56
lecture

***


Coding th

