## Lecture 4: Basic Introduction to Transformers
1. __Pre-training__: Training on a large-diverse dataset
2. __Fine-tuning__: Refinement-by-training on a narrower-dataset, specific to a particular task or domain.

#### Transformer Architecture
* Most of the modern LLMs rely on this architecture, which is called as Transformer architecture.
* Essentially it's a DNN architecture, where basically we're trying to optimize the parameters, which was introduced in 2017 paper.

#### [Attention is all you need](https://arxiv.org/abs/1706.03762)
*  __Translation__: English-to-French and English-to-German.
*  Text completion, which is the pre-dominant role of GPT was not even in consideration here.
*  GPT architecture, which is the foundational stone or foundational building-block of ChatGPT, originated from this paper.

#### What is attention?
* It is actually a technical term, which is related to how attention is used in our daily life.

* __Embedding tokenization__


#### Schematic of the Transformer

1. __Input text__, which is to be translated. "This is an example".

2. __Pre-Processing__: There is process (1) breaks-down a sentence into words or tokens (called __Tokenization__) and then (2) __assigns an ID a unique number__ to each token. For simplicity, you can imagine that one-word is one-token.

3. __Encoder__: Encoder performs __Vector embedding__, which converts the words into vectorized representation. NNs are trained for this step. Giant dimension-space. This vectorized representation captures the __semantic meaning__ between the words. Basically, encode-the-information somewhere such that similar words are related to each other. The words are projected into high-dimensional __Vector space__ and the way these words are projected is such that the __semantic relationship__ or the semantic meaning between the words is captured very clearly.

4. __Decoder__: The docoder receives 1) partial-input text, and 2) the vector embeddings. It has to predict what the next word is going to be based on this information. The partial output text remember this is available to the model because. The model only generates one output word at a time.

5. Generate the translated __text one word at a time__.

6. __Final output__: Complete sentence. It's like a NN and we are training the NN. Initially it will make mistakes of course but there will be a __loss-function__ and then we will eventually train the Transformer to be better-and-better. Think of the this as a NN. __Feed-forward layers__, which means there are __weights and parameters which need to be optimized__.

***

### Transfomer
* Transformer as a neural network and you're optimizing parameters in a neural network.
* Encoder: Encodes intput text into vectors
* Decoder: Generates out text from encoded vectors

* __attention__:  

* Main purpose of the encoder is to convert the input text into embedding vectors great and the main purpose of the decoder is to generate the output text from the embedding-vectors and from the partial-output, which it has received.

* GPT architecture is actually different than the Transformer because that came later and it does not have the encoder it only has the decoder.
*

* __A note on attention__ let's actually Google or let's actually control F attention here and see how many times it shows up 97 times and let's see how they have defined attention actually uh okay attention mechanisms have become an integral part of sequence modeling allowing modeling of dependencies without regard to their distance in the input or output sequences remember this so the attention mechanism allows you to model the dependencies between different words without regards to how close apart or how far apart the words are that is one key thing to remember uh and then self attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence this is a bit difficult to understand so let me actually explain to you the way I understood it on the white board what basically self attention mechanism does is that or attention is that it allows the model to weigh the importance of different words and tokens relative to each other so let's say you have two sentences right and uh let's say the first sentence is Harry Potter is on station or platform number something and then Harry Potter wants to board the train and then third sentence for fourth sentence when you are on the fourth sentence to predict the next word in the fourth sentence the context is very important right so you need to know what the text was in the sentence number one sentence number two and sentence number three as well only then you will be able to really understand the fourth sentence and predict the next word in the fourth sentence this is the meaning of __long-range dependencies__, which means that if I'm predicting the next word in the fourth sentence I need to know the importance of previous words I need to know how much attention should I give to the previous Words which word which has come previously should receive more attention maybe Harry should receive more attention maybe platform or train should receive more attention the self attention mechanism allows you to capture long range dependencies so that the model can look even far behind and even to sentences closer to the current one to identify the next one to identify the next word so the self attention mechanism allows the model to weigh the importance of different words or tokens relative to each other that is very important so basically uh if you are to predict the next word the self attention mechanism maintains this attention score which basically tells you which word should be given more attention when you predict the next word and this is a key part of the intuition for all of you to think about so let us actually look at uh this architecture and look at the part where attention comes in see __multi-head attention__ mask multi-head attention there are these blocks which are called as __attention blocks__ so these attention blocks make sure you capture long-range dependencies in the sentences that's why this paper is actually called attention is all you need because of the __self attention mechanism__ uh and the intuition behind attention which these folks introduced so as I mentioned they they calculate an attention score which basically it's a matrix and it tells you which words should be given more importance in relative or in relation to other words for now just understand this intuition so that later when we come to the mathematics and coding of it I just want you to be comfortable with this notion and I want you to appreciate how beautiful this is because you as a human we keep context into our mind pretty naturally when we are reading a story we remember what was written on the previous page but for a model to do that it's very difficult and self attention mechanism actually allows the model to do that it allows the model to capture long-range effect dependencies so thatit it makes the next word prediction accurately in chat GPT.

* when you write an input GPT actually gives attention to every sentence right and then it predicts what the next word could be it doesn't just look at the sentence before the current one it looks at all sent sentences because maybe previous words are more important this is possible through the self attention mechanism so that's the second key concept which I wanted to introduce and uh in the last part of the lecture we are going to look at the __BERT and GPT__ later variations of the Transformer architecture so the Transformer architecture or this paper rather cameout in 2017 right the GPT models came out after that and there's another set of models called as __Bert__ which also came out as l variations of the Transformer architecture so there are two later variations which I want to discuss the first is called as B it's called as by or its full form is __B directional encoder__ representations from Transformers no need to understand the meaning but that's just what b means maybe you would have heard this terminology but did not know the full form the full form is B directional encoder representations from Transformers and the second is GPT models of course all of us have heard of chat GPT but the full form of __GPT is generative pre-trained Transformers__ uh pre-trained because it's a pre-trained or a foundational model which we saw in the previous lecture so now you should start understanding these terminologies now you must be thinking what's the difference between Bert and GPT models there's a big difference basically the way Bert operates is that it predicts hidden words in a given sentence so let's say you have a sentence it will mask some words randomly and it will try to predict those mask or hidden words that's what B does what does GPT do as we have all seen it generates a new word so there is a pretty big difference between how B works and how GPT Works let's see a schematic so this is how bir actuallyworks. let's say we have a sentence this is an Dash or question mark of how llm Dash perform so let's say as input we have this a text which is incomplete so Bert receives inputs where words are randomly masked during training and mask means that let's say we do not know these words right and then this is the input text we do the same pre-processing steps as we saw above converting them into uh token IDs then we pass it to the encoder do the embedding same thing like what we saw before and then the main output is that we fill the missing words so Bert model realizes that the missing words are example and can so then the final sentence is this is an example of how llm can perform this is how B Works how GPT actually works is something completely different so let's say the input which GPT receives is this is an example of how llm can Dash so we just have to predict the next World we receive incomplete text and we have to predict the next word so then the way GPT works is that it does the pre-processing like we saw converts words to token IDs then there is a decoder model there is not an encoder model so the decoder then predicts the last word or the word which we do not know perform so then the output is this is an example of how llm can perform so GPT models learn to generate one word at a time now this leads to Big differences because if you see the GPT model is left to right right all the left information is there we are only predicting the rightmost information what is not known whereas in bir random words can be masked so the model has to pay attention to different parts of the sentence and that's why ber actually does very well in sentiment analysis so here I
have I have a text which actually Compares uh or answers why is Bert so good when we do uh sentiment analysis so the reason Bert is called B directional so you'll see that the name is by directional encoder representations right because it looks at the sentence from both directions whereas in GPT we just look at the sentence from left to right but ber looks at the sentence from both directions because even the first word might be missing so it has to look from the left side as well as the right side and by looking at the entire sentence from both directions bir can capture the nuances and relationships between words that are important for for understanding meaning and context for instance Bert model can differentiate between Bank as a financial institution and bank as a River Bank by looking at the surrounding words so that is why ber can actually understand the nuances and relationships between different words in a sentence since it looks at the sentence from both sides that's why Bert is very commonly used in sentiment analysis even GPT can be used in sentiment analysis but the speciality of birth is sentiment analysis okay so that's the difference between bir and GPT no one talks too much about bir these days all the hype is about GPT chat GPT because it produces one word at a time it can complete missing text also it can also do sentiment analysis uh but I just want to explain I just wanted to explain these differences to you so that you you are aware of what BT means what GPT means one thing to not is that both of these have the word transform forers in them because they have originated from the Transformer architecture here uh which we just consider but as you saw the GPT model does not have an encoder they only have a decoder that's one key thing which I wanted to demonstrate here whereas.

* The Bert model only has the encoder so remember these differences between Bert and GPT great and now one thing which I Difference between transformers and LLMs would like to cover before we end the lecture is that what is the difference between Transformer and llm so are they the same thing when we say llms can we also say Transformers so the key thing to note is that not all Transformers are llms Transformers can also be used for other tasks like computer vision so one thing which I would like to show you here is this thing so Transformers are not only used for language tasks they are also used for vision tasks such as image recognition image classification Etc here is a website which I have pulled out the these are called as Vision Transformers vit and they can be used for various application so here see the vision Transformer is being used to detect a PO hole on the road then there are a number of other important application such as it can be used to classify between tumors as maligant and venine just from the images and a number of people have discuss the similarities and differences between convolutional neural network and viit so viit AES remarkable results compared to CNN while obtaining substantially fewer computational resources for pre-training in comparison to C CNN Vision Transformers show a generally weaker bias so basically you think of only convolutional neural networks when you think of image classification right but Vision Transformers are a new method which is also gaining a lot of popularity and they can be used for image classification tasks so remember when you think of Transformers don't think of Transformers only in the context of large language models or text generation Transformers can also be used for computer vision so remember not all Transformers are llms so what about llms are all llms Transformers so that is also not true not all llms are also Transformers llms can be based on recurrent or convolutional architectures as well this is what very important point to remember

* I had made a presentation uh some time back and this image has been taken from stats Quest channel so before even Transformers came into the picture here you can see 1980 recurrent neural networks were introduced in 1997 long short-term memory networks were introduced both of them could do sequence modeling tasks and both of them could do text completion tasks so they also can be called as language models so remember that all llms are not uh Transformers right llms can also be recurrent neural networks or long shortterm memory networks to give you a quick introduction what RNN actually do is that RNN maintain this kind of a feedback loop so that is why we can incorporate memory into account uh lstm on the other hand incorporates two separate paths One path about short-term memories and one path about long-term memories that's why they are called long short-term um memory networks so One path is for long-term memories and one path is for short-term memories so basically we have one green line let's say which is shown here that represents long-term memory one line which shows the short-term memory and then basically using both we can make predictions of what comes next so even recurrent neural networks and long short-term memory Networks and even some convolutional architectures can also be large language models so as we end I just want you to remember that not all Transformers are llms this is very important to keep in mind and not all llms are Transformers also so don't use the terms Transformers and llms interchangeably they are actually very different things but not many students or not many people really understand the similarities or the differences between them one purpose of these set of lectures is for you to understand everything from Basics the way it is supposed to be that way you'll also be much more confident when you transition your career or you're sitting for an llm interview and if you don't know the difference between Transformers or llms these lectures can clarify those similarities and differences for you I'm going to go into a lot of detail in lectures like what we did right now and not assume anything so I've written number of things on the Whiteboard so that you can understand let's do a quick recap of what all we learned first we saw that most modern llms rely on the Transformer architecture which was proposed in the 2017 paper it's basically a deep neural network architecture the paper which proposed the Transformer architecture is called as attention is all you need and the original Transformer was developed for machine translation for translating English tasks or English texts into German and French we saw a simplified Transformer architecture which had eight steps we take an input example pre-process it by converting words or sentences into words and token IDs then we pass it into the encoder which converts these tokens into Vector embeddings the vector embeddings are fed to the decoder along with the vector embeddings the decoder also receives partial output text and it generates the translated sentence one word at a time this is the simplified Transformer architecture and we saw that the Transformer architecture consists of an encoder and a decoder however later we saw that GPT models do not have an encoder they only have a decoder in the middle we had a small discussion on self attention mechanism which is really the heart of why Transformers works so well and why the paper which I showed you earlier is called attention is all you need self attention allows the model to weigh the importance of different words relative to each other and it enables the model to capture long range dependencies so when we are predicting the next word from a given sentence we can look at all

* the context in the past and way the importance of which word matters more for predicting the next word you can think of also self attention as parallel attention to different parts of a paragraph or different sentences we will look into this later it's going to be one of the key aspects uh it's going to be one of the key aspects of our course as we move forward then we saw the later variations of transform Transformer architecture in particular we looked at two variations first is B which is B directional encoder representations and then we saw GPT so there is a difference between these right Bert predicts hidden hidden words in a sentence or it predicts missing Words which are also called masked words so basically what this does is that word pays attention to a sentence from left side as well as from the right side because any word can be masked that's why it's called B directional encoder and it does not have the decoder architecture it just has the encoder architecture since ber looks at a sentence from both the words both the directions it can capture the meanings of different words and how they relate to each other very well and that's why BT models are used for sentiment analysis A Lot GPT on the other hand just gets the data and then it predicts the next word so it's it's a left to right model basically it has data from the left hand side and then it has to predict what comes on the right or what's the next work so GPT receives incomplete text and learns to generate one word at a time um and main thing to remember is that GPT does not have an encoder it only has decoder great and then in the last part of the lecture we saw the difference between Transformers versus llms so remember not all Transformers are llms Transformers can also be used for computer vision tasks like image classification image segmentation Etc similarly not all llms are Transformers before Transformers came recurrent neural networks and long short-term memory networks and even convolutional architectures were used for text completion so that's why llms can be based on recurrent or convolutional architectures as well so do not use these terms Transformers and llms interchangeably though many people do it understand the similarities and differences between the two that brings us to the end of this lecture we covered a lot of we covered Five Points in today's lecture and.

* I encourage you to be proactive in the comment section ask questions ask doubts uh also make notes about these architectures as you are as you are learning that's really one of the best ways to learn about this material and as always I try to show everything on a whiteboard plus try to explain as clearly as possible so that nothing is left out and I show a lot of examples also in this process thanks a lot everyone I hope you are enjoying in this series I look forward to seeing you in the next lecture

***

