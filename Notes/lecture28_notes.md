#### 
1. Text Generation
2. Text Evaluation
3. Training & Validation Losses
4. LLM Training Function

***

* 15:00

* __Main step__: Find loss gradients using `loss.backward()`

* Input --> GPT model --> Logits --> Softmax --> Cross Entropy Loss (Output, Target)

* 161M parameters
  
***

* 25:00

* ADAM

***

* 30:00
  
* AdamW Optimizer

***

