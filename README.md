#### Books
* [Book Links](https://github.com/muarshad01/LLM/blob/main/books.md)

*** 

#### Installation
* [Installation on MacOS](https://github.com/muarshad01/LLM/blob/main/Installation.md)

***

#### Quick Steps to Run NoteBook
```unix
$ cd ~/Desktop/LLMs-from-scratch
$ source .venv/bin/activate
$ uv run jupyter lab
```

***

#### Building LLMs From Scratch
* [GitHub Link for Book - Code](https://github.com/rasbt/LLMs-from-scratch)
* [ALL Video Lectures](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)

***

| Lecture | Notes | Date Updated |
|---|---|---|
| [Lecture 01 -- Building LLMs from scratch: Series introduction](https://www.youtube.com/watch?v=Xpr8D6LeAtw) |  [Notes01](https://github.com/muarshad01/LLM/blob/main/Notes/lecture01_notes.md)| 01/01/2026 |
| [Lecture 02 -- LLM basics](https://www.youtube.com/watch?v=3dWzNZXA8DY)| [Notes02](https://github.com/muarshad01/LLM/blob/main/Notes/lecture02_notes.md) | 01/01/2026 |
| [Lecture 03 -- Pretraining LLMs vs finetuning LLMs](https://www.youtube.com/watch?v=-bsa3fCNGg4)|  [Notes03](https://github.com/muarshad01/LLM/blob/main/Notes/lecture03_notes.md) | 01/01/2026 |
| [Lecture 04 -- What are transformers?](https://www.youtube.com/watch?v=NLn4eetGmf8) |  [Notes04](https://github.com/muarshad01/LLM/blob/main/Notes/lecture04_notes.md) | 01/01/2026 |
| [Lecture 05 -- How does GPT-3 really work?](https://www.youtube.com/watch?v=xbaYCf2FHSY) |  [Notes05](https://github.com/muarshad01/LLM/blob/main/Notes/lecture05_notes.md) | 01/01/2026 |
| [Lecture 06 -- Stages of building an LLM from scratch](https://www.youtube.com/watch?v=z9fgKz1Drlc) |  [Notes06](https://github.com/muarshad01/LLM/blob/main/Notes/lecture06_notes.md) | 01/01/2026 |
| [Lecture 07 -- Code an LLM tokenizer from scratch in Python](https://www.youtube.com/watch?v=rsy5Ragmso8) |  [Notes07](https://github.com/muarshad01/LLM/blob/main/Notes/lecture07_notes.md) | 05/01/2026 |
| [Lecture 08 -- The GPT tokenizer: Byte Pair Encoding (BPE)](https://www.youtube.com/watch?v=fKd8s29e-l4) |  [Notes08](https://github.com/muarshad01/LLM/blob/main/Notes/lecture08_notes.md) | 05/01/2026 |
| [Lecture 09 -- Creating Input-Target data pairs using Python DataLoader](https://www.youtube.com/watch?v=iQZFH8dr2yI) |  [Notes09](https://github.com/muarshad01/LLM/blob/main/Notes/lecture09_notes.md) | 06/01/2026 |
| [Lecture 10 -- What are Token Embeddings?](https://www.youtube.com/watch?v=ghCSGRgVB_o) |  [Notes10](https://github.com/muarshad01/LLM/blob/main/Notes/lecture10_notes.md) | 07/01/2026 |
| [Lecture 11 -- The importance of Positional Embeddings](https://www.youtube.com/watch?v=ufrPLpKnapU) |  [Notes11](https://github.com/muarshad01/LLM/blob/main/Notes/lecture11_notes.md) | 08/01/2026 |
| [Lecture 12 -- The entire Data Preprocessing Pipeline of LLMs](https://www.youtube.com/watch?v=mk-6cFebjis) |  [Notes12](https://github.com/muarshad01/LLM/blob/main/Notes/lecture12_notes.md)| |
|---|---|
| [Lecture 13 -- Introduction to the Attention Mechanism in LLMs](https://www.youtube.com/watch?v=XN7sevVxyUM) |  [Notes13](https://github.com/muarshad01/LLM/blob/main/Notes/lecture13_notes.md) | 12/01/2026 |
| [Lecture 14 -- Simplified Attention Mechanism - Coded from scratch in Python - No trainable weights](https://www.youtube.com/watch?v=eSRhpYLerw4) |  [Notes14](https://github.com/muarshad01/LLM/blob/main/Notes/lecture14_notes.md) | 13/01/2026 |
| [Lecture 15 -- Coding the Self Attention Mechanism with key, query and value (K, Q, V) matrices](https://www.youtube.com/watch?v=UjdRN80c6p8) |  [Notes15](https://github.com/muarshad01/LLM/blob/main/Notes/lecture15_notes.md) | 14/01/2026 |
| [Lecture 16 -- Causal Self Attention Mechanism - Coded from scratch in Python](https://www.youtube.com/watch?v=h94TQOK7NRA) |  [Notes16](https://github.com/muarshad01/LLM/blob/main/Notes/lecture16_notes.md) | 15/01/2026 |
| [Lecture 17 -- Multi Head Attention Part 1 - Basics and Python code](https://www.youtube.com/watch?v=cPaBCoNdCtE) |  [Notes17](https://github.com/muarshad01/LLM/blob/main/Notes/lecture17_notes.md) | 15/01/2026 |
| [Lecture 18 -- Multi Head Attention Part 2 - Entire mathematics explained](https://www.youtube.com/watch?v=K5u9eEaoxFg) |  [Notes18](https://github.com/muarshad01/LLM/blob/main/Notes/lecture18_notes.md) | 15/01/2026 |
|---|---|
| [Lecture 19 -- Birds Eye View of the LLM Architecture](https://www.youtube.com/watch?v=4i23dYoXp-A) |  [Notes19](https://github.com/muarshad01/LLM/blob/main/Notes/lecture19_notes.md) | 16/01/2026 |
| [Lecture 20 -- Layer Normalization in the LLM Architecture](https://www.youtube.com/watch?v=G3W-LT79LSI) |  [Notes20](https://github.com/muarshad01/LLM/blob/main/Notes/lecture20_notes.md) | 17/01/2026 |
| [Lecture 21 -- GELU Activation Function in the LLM Architecture](https://www.youtube.com/watch?v=d_PiwZe8UF4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21) |  [Notes21](https://github.com/muarshad01/LLM/blob/main/Notes/lecture21_notes.md) | 18/01/2026 |
| [Lecture 22 -- Shortcut connections in the LLM Architecture](https://www.youtube.com/watch?v=2r0QahNdwMw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=22) |  [Notes22](https://github.com/muarshad01/LLM/blob/main/Notes/lecture22_notes.md) | 18/01/2026 |
| [Lecture 23 -- Coding the entire LLM Transformer Block](https://www.youtube.com/watch?v=dvH6lFGhFrs&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=23) |  [Notes23](https://github.com/muarshad01/LLM/blob/main/Notes/lecture23_notes.md) | 19/01/2026 |
| [Lecture 24 -- Coding the 124 million parameter GPT-2 model](https://www.youtube.com/watch?v=G3-JgHckzjw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=24) |  [Notes24](https://github.com/muarshad01/LLM/blob/main/Notes/lecture24_notes.md) | 20/01/2026 |
| [Lecture 25 -- Coding GPT-2 to predict the next token](https://www.youtube.com/watch?v=F1Sm7z2R96w&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=25) |  [Notes25](https://github.com/muarshad01/LLM/blob/main/Notes/lecture25_notes.md) | 21/01/2026 |
|---|---|---|
| [Lecture 26 -- Measuring the LLM loss function](https://www.youtube.com/watch?v=7TKCrt--bWI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=26) |  [Notes26](https://github.com/muarshad01/LLM/blob/main/Notes/lecture26_notes.md) | 22/01/2026 |
| [Lecture 27 -- Evaluating LLM performance on real dataset - Hands on project - Book data](https://www.youtube.com/watch?v=zuj_NJNouAA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=27) |  [Notes27](https://github.com/muarshad01/LLM/blob/main/Notes/lecture27_notes.md) ||
|---|---|---|
| [Lecture 28 -- Coding the entire LLM Pre-training Loop](https://www.youtube.com/watch?v=Zxf-34voZss&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=28) |  [Notes28](https://github.com/muarshad01/LLM/blob/main/Notes/lecture28_notes.md) ||
| [Lecture 29 --Temperature Scaling in Large Language Models (LLMs)](https://www.youtube.com/watch?v=oG1FPVnY0pI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=29) |  [Notes29](https://github.com/muarshad01/LLM/blob/main/Notes/lecture29_notes.md) ||
| [Lecture 30 -- Top-k sampling in Large Language Models](https://www.youtube.com/watch?v=EhU32O7DkA4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=30) |  [Notes30](https://github.com/muarshad01/LLM/blob/main/Notes/lecture30_notes.md) ||
| [Lecture 31 -- Saving and loading LLM model weights using PyTorch](https://www.youtube.com/watch?v=Bc-9sf0VihQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=31) |  [Notes31](https://github.com/muarshad01/LLM/blob/main/Notes/lecture31_notes.md) ||
| [Lecture 32 -- Loading pre-trained weights from OpenAI GPT-2](https://www.youtube.com/watch?v=yXrGeDNuymY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=32) |  [Notes32](https://github.com/muarshad01/LLM/blob/main/Notes/lecture32_notes.md) ||
|---|---|---|
| [Lecture 33 -- Introduction to LLM Finetuning - Python Coding with hands-on-example](https://www.youtube.com/watch?v=yZpy_hsC1bE&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=33) |  [Notes33](https://github.com/muarshad01/LLM/blob/main/Notes/lecture33_notes.md) ||
| [Lecture 34 -- Dataloaders in LLM Classification Finetuning - Python Coding - Hands on LLM project](https://www.youtube.com/watch?v=f6zqClXOh7Y&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=34) |  [Notes34](https://github.com/muarshad01/LLM/blob/main/Notes/lecture34_notes.md) ||
| [Lecture 35 -- Coding the model architecture for LLM classification fine-tuning](https://www.youtube.com/watch?v=izyxvl-2JlM&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=35) |  [Notes35](https://github.com/muarshad01/LLM/blob/main/Notes/lecture35_notes.md) ||
| [Lecture 36 -- Coding a fine-tuned LLM spam classification model - From Scratch](https://www.youtube.com/watch?v=0PpxZ3kNPWo&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=36) |  [Notes36](https://github.com/muarshad01/LLM/blob/main/Notes/lecture36_notes.md) ||
| [Lecture 37 -- Introduction to LLM Instruction Fine-tuning - Loading Dataset - Alpaca Prompt format](https://www.youtube.com/watch?v=1bLhvqZzdaQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=37) |  [Notes37](https://github.com/muarshad01/LLM/blob/main/Notes/lecture37_notes.md) ||
| [Lecture 38 -- Data Batching in LLM instruction fine-tuning - Hands on project - Live Python coding](https://www.youtube.com/watch?v=bkUkcyL_Xxc&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=38) |  [Notes38](https://github.com/muarshad01/LLM/blob/main/Notes/lecture38_notes.md) ||
| [Lecture 39 -- Dataloaders in Instruction Fine-tuning](https://www.youtube.com/watch?v=egFqsJQ9kuY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=39) |  [Notes39](https://github.com/muarshad01/LLM/blob/main/Notes/lecture39_notes.md) ||
| [Lecture 40 -- Instruction fine-tuning: Loading pre-trained LLM weights](https://www.youtube.com/watch?v=__OiQznq4ao&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=40) |  [Notes40](https://github.com/muarshad01/LLM/blob/main/Notes/lecture40_notes.md) ||
| [Lecture 41 -- LLM fine-tuning training loop - Coded from scratch](https://www.youtube.com/watch?v=r7unILsP0Es&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=41) |  [Notes41](https://github.com/muarshad01/LLM/blob/main/Notes/lecture41_notes.md) ||
| [Lecture 42 -- Evaluating fine-tuned LLM using Ollama](https://www.youtube.com/watch?v=7m2jV7BOFkA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=42) |  [Notes42](https://github.com/muarshad01/LLM/blob/main/Notes/lecture42_notes.md) ||
| [Lecture 43 -- Build LLMs from scratch 20 minutes summary](https://www.youtube.com/watch?v=_xH-jXNFRjA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=43) |  [Notes43](https://github.com/muarshad01/LLM/blob/main/Notes/lecture43_notes.md) ||

***

## Deep Learning
|Lecture | Notes|
|---|---|
| [Deep Learning Chapter 1 -- But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) ||
| [Deep Learning Chapter 2 -- Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w) ||
| [Deep Learning Chapter 3 -- Backpropagation, intuitively](https://www.youtube.com/watch?v=Ilg3gGewQ5U) ||
| [Deep Learning Chapter 4 -- Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8) ||
| [Deep Learning Chapter 5 -- Transformers, the tech behind LLMs](https://www.youtube.com/watch?v=wjZofJX0v4M) ||
| [Deep Learning Chapter 6 -- Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc) ||
| [Deep Learning Chapter 7 -- How might LLMs store facts ](https://www.youtube.com/watch?v=9-Jl0dxWQs8) ||

***

* [The Most Important Algorithm in Machine Learning](https://www.youtube.com/watch?v=SmZmBKc7Lrs)
* [[Deep Learning 101 - Cross-Entropy Loss Function Demystified](https://www.youtube.com/watch?v=FODwUM-1PyI)
* [A Review of 10 Most Popular Activation Functions in Neural Networks](https://www.youtube.com/watch?v=56ZxEmGRt2k)
* [Understanding Large Language Model - Under The Hood](https://www.youtube.com/playlist?list=PLUfbC589u-FSwnqsvTHXVcgmLg8UnbIy3)
* [How Attention Mechanism Works in Transformer Architecture](https://www.youtube.com/watch?v=KMHkbXzHn7s)
* [Generative Machine Learning - Attention Mechanisms with Math](https://www.youtube.com/playlist?list=PLs8w1Cdi-zvalz9ltXmarqyeQ49wfKFqf)
* [AGI Lambda](https://www.youtube.com/@AGI.Lambdaa/shorts)
* [Vision Transformers](https://www.youtube.com/shorts/qPUYBX0C6ic)
* [How RNNs Help AI Understand Language](https://www.youtube.com/shorts/w67EHFHGHUQ)
* [How does NN work in 60 seconds](https://www.youtube.com/shorts/Dbcx2_MO0LM)
* [BERT Networks in 60 seconds](https://www.youtube.com/shorts/HBOloY08auQ)
* [What is RAG](https://www.youtube.com/shorts/CbAQUqnrDcA)
* [MCP Protocol](https://www.youtube.com/shorts/7CHr0qwTcJw)
* [Build a Small Language Model (SLM) From Scratch](https://www.youtube.com/watch?v=pOFcwcwtv3k)
* [Large Language Models explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs&t=2s)
* [What is Tensor](https://www.youtube.com/shorts/J4Tg4gAPMMQ)
* [How word vectors encode meaning](https://www.youtube.com/shorts/FJtFZwbvkI4)
* [Self-Attention in Transformer](https://www.youtube.com/shorts/l8_OrR9kUNw)
* [I Visualised Attention in Transformers](https://www.youtube.com/watch?v=RNF0FvRjGZk)
* Autoencoders | Deep Learning Animated
* [The Most Important Algorithm in Machine Learning](https://www.youtube.com/watch?v=SmZmBKc7Lrs)
* [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)
* [ML Foundations for AI Engineers (in 34 Minutes)](https://www.youtube.com/watch?v=BUTjcAjfMgY)
* [How DeepSeek Rewrote the Transformer [MLA]](https://www.youtube.com/watch?v=0VLAoVGf_74)

***

#### [Open Source LLM Tools - Chip Huyen](https://huyenchip.com/llama-police.html)

***

#### [What's The Difference Between Matrices And Tensors?](https://www.youtube.com/watch?v=1GwAEnegaRs)

#### [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)

***

#### TODO
1. [Build DeepSeek from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms)
2. [Reasoning LLMs from Scratch (Reinforcement Learning)](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSijcbUrRZHm6BrdinLuelPs)
3. [Neural Networks from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSj6tNyn_UadmUeU3Q3oR-hu)

***

#### AI/ML Companies
* [ai71 - UAE](https://ai71.ai/careers)
* [HUMAIN - Saudi Arabia](https://www.humain.com/)
  * Phase-1: 25,000 NVIDIA H100 GPUS (world's most advanced)
* [QAI - Qatar](https://www.qailab.qa/)

***
