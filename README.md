## LLM (Dr. Raj Dandekar)
| Lecture | Notes | Date|
|---|---|---|
| [Lecture 01: Building LLMs from scratch: Series introduction](https://www.youtube.com/watch?v=Xpr8D6LeAtw) | Aug , 2025|
* [Lecture 02: Large Language Models (LLM) Basics](https://www.youtube.com/watch?v=3dWzNZXA8DY)
* [Lecture 03: Pretraining LLMs vs Finetuning LLMs](https://www.youtube.com/watch?v=-bsa3fCNGg4)
* [Lecture 04: What are transformers?](https://www.youtube.com/watch?v=NLn4eetGmf8)
* [Lecture 05: How does GPT-3 really work?](https://www.youtube.com/watch?v=xbaYCf2FHSY)
* [Lecture 06: Stages of building an LLM from Scratch](https://www.youtube.com/watch?v=z9fgKz1Drlc)
* [Lecture 07: Code an LLM Tokenizer from Scratch in Python](https://www.youtube.com/watch?v=rsy5Ragmso8)
* [Lecture 08: The GPT Tokenizer: Byte Pair Encoding](https://www.youtube.com/watch?v=fKd8s29e-l4)
* [Lecture 09: Creating Input-Target data pairs using Python DataLoader](https://www.youtube.com/watch?v=iQZFH8dr2yI)
* [Lecture 10: What are token embeddings?](https://www.youtube.com/watch?v=ghCSGRgVB_o)
* [Lecture 11: The importance of Positional Embeddings](https://www.youtube.com/watch?v=ufrPLpKnapU)
* [Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs)](https://www.youtube.com/watch?v=mk-6cFebjis)
* [Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs)](https://www.youtube.com/watch?v=XN7sevVxyUM)
* [Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python | No trainable weights](https://www.youtube.com/watch?v=eSRhpYLerw4)
* [Lecture 15: Coding the self attention mechanism with key, query and value matrices](https://www.youtube.com/watch?v=UjdRN80c6p8)
* [Lecture 16: Causal Self Attention Mechanism | Coded from scratch in Python](https://www.youtube.com/watch?v=h94TQOK7NRA)
* [Lecture 17: Multi Head Attention Part 1 - Basics and Python code](https://www.youtube.com/watch?v=cPaBCoNdCtE)
* [Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained](https://www.youtube.com/watch?v=K5u9eEaoxFg)
* [Lecture 19: Birds Eye View of the LLM Architecture](https://www.youtube.com/watch?v=4i23dYoXp-A)
* [Lecture 20: Layer Normalization in the LLM Architecture](https://www.youtube.com/watch?v=G3W-LT79LSI)

***

#### [Transformers (how LLMs work) explained visually | DL5](https://www.youtube.com/watch?v=wjZofJX0v4M)

***
