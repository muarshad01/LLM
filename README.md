## LLM (Dr. Raj Dandekar)
| Lecture | Notes | Date|
|---|---|---|
| [Lecture 01: Building LLMs from scratch: Series introduction](https://www.youtube.com/watch?v=Xpr8D6LeAtw) | ... | Aug , 2025|
| [Lecture 02: Large Language Models (LLM) Basics](https://www.youtube.com/watch?v=3dWzNZXA8DY)| [Notes01](https://github.com/muarshad01/LLM/blob/main/Notes/lecture01_notes.md) | Aug , 2025|
| [Lecture 03: Pretraining LLMs vs Finetuning LLMs](https://www.youtube.com/watch?v=-bsa3fCNGg4)| ... | Aug , 2025|
| [Lecture 04: What are transformers?](https://www.youtube.com/watch?v=NLn4eetGmf8) | ... | Aug , 2025|
| [Lecture 05: How does GPT-3 really work?](https://www.youtube.com/watch?v=xbaYCf2FHSY) | ... | Aug , 2025|
| [Lecture 06: Stages of building an LLM from Scratch](https://www.youtube.com/watch?v=z9fgKz1Drlc) | ... | Aug , 2025|
| [Lecture 07: Code an LLM Tokenizer from Scratch in Python](https://www.youtube.com/watch?v=rsy5Ragmso8) | ... | Aug , 2025|
| [Lecture 08: The GPT Tokenizer: Byte Pair Encoding](https://www.youtube.com/watch?v=fKd8s29e-l4) | ... | Aug , 2025|
| [Lecture 09: Creating Input-Target data pairs using Python DataLoader](https://www.youtube.com/watch?v=iQZFH8dr2yI) | ... | Aug , 2025|
| [Lecture 10: What are token embeddings?](https://www.youtube.com/watch?v=ghCSGRgVB_o) | ... | Aug , 2025|
| [Lecture 11: The importance of Positional Embeddings](https://www.youtube.com/watch?v=ufrPLpKnapU) | ... | Aug , 2025|
| [Lecture 12: The entire Data Preprocessing Pipeline of Large Language Models (LLMs)](https://www.youtube.com/watch?v=mk-6cFebjis) | ... | Aug , 2025|
| [Lecture 13: Introduction to the Attention Mechanism in Large Language Models (LLMs)](https://www.youtube.com/watch?v=XN7sevVxyUM) | ... | Aug , 2025|
| [Lecture 14: Simplified Attention Mechanism - Coded from scratch in Python - No trainable weights)](https://www.youtube.com/watch?v=eSRhpYLerw4) | ... | Aug , 2025|
| [Lecture 15: Coding the self attention mechanism with key, query and value matrices](https://www.youtube.com/watch?v=UjdRN80c6p8) | ... | Aug , 2025|
| [Lecture 16: Causal Self Attention Mechanism - Coded from scratch in Python](https://www.youtube.com/watch?v=h94TQOK7NRA) | ... | Aug , 2025|
| [Lecture 17: Multi Head Attention Part 1 - Basics and Python code](https://www.youtube.com/watch?v=cPaBCoNdCtE) | ... | Aug , 2025|
| [Lecture 18: Multi Head Attention Part 2 - Entire mathematics explained](https://www.youtube.com/watch?v=K5u9eEaoxFg) | ... | Aug , 2025|
| [Lecture 19: Birds Eye View of the LLM Architecture](https://www.youtube.com/watch?v=4i23dYoXp-A) | ... | Aug , 2025|
| [Lecture 20: Layer Normalization in the LLM Architecture](https://www.youtube.com/watch?v=G3W-LT79LSI) | ... | Aug , 2025|

***

#### [Transformers (how LLMs work) explained visually | DL5](https://www.youtube.com/watch?v=wjZofJX0v4M)

***
