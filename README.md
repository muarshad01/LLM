#### Book
* [Book: Build a Large Language Model (From Scratch) by Sebastian Raschka](https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167/ref=sr_1_1?adgrpid=186568885419&dib=eyJ2IjoiMSJ9.4aksU58BPWhOPrzkY0x_-vygt-ueYbeZO3NZCcItOJhwUkUI29dKBu191StjAX_6kFIlMZDQaXqiikWYV4yCxJyCN-1fORiR8HXdYk1pnOy5pMuFLCLhD5oeLTs9Ckz_Gnoe1ZnkxkuYsSetETUZjciRj-PrGLoJVEoGdjR8eqwYDRrYz3VpxBUSY6KP-X8vtpcM45C3qypNE1aMCVFd5L0U8ZmbFp0Tl28wzFU-cxg.P3ksogW4khJ39syyd4KIpkFJva8xg6NeQUZtKnKBgbM&dib_tag=se&hvadid=779542975090&hvdev=c&hvexpln=0&hvlocphy=9008257&hvnetw=g&hvocijid=12364010510054860427--&hvqmt=b&hvrand=12364010510054860427&hvtargid=kwd-2192499717731&hydadcr=16405_13751106_11567&keywords=build+an+llm+from+scratch&mcid=c7100230a49531d7af7d4236df7c0991&qid=1767193721&sr=8-1)
* [Book - Build a Reasoning Model (From Scratch) by Sebastian Raschka](https://www.amazon.com/Build-Reasoning-Scratch-Sebastian-Raschka/dp/1633434672/ref=books_amazonstores_desktop_mfs_aufs_ap_sc_dsk_0?_encoding=UTF8&pd_rd_w=zMkKG&content-id=amzn1.sym.6d92b4c0-97d6-4063-b66e-20890dfbd616&pf_rd_p=6d92b4c0-97d6-4063-b66e-20890dfbd616&pf_rd_r=134-4549822-7099125&pd_rd_wg=B5Q4A&pd_rd_r=2a487d40-aa7b-4d23-bc7c-39bc39664b9a)
* [Book - Build a DeepSeek Model (From Scratch) Raj Abhijit Dandekar et al.](https://www.amazon.com/Build-DeepSeek-Scratch-Abhijit-Dandekar/dp/163343432X/ref=sr_1_1?crid=2LUC6MVW83U8S&dib=eyJ2IjoiMSJ9.eQJGSpzHuCjoIweVeAcJ_PtXrSaFbaF7k9s0AIXyp990jsrxFUCNXde2CiflJn3hRT0_2b2hdDEDZX9baXwEBkLufGK4la0nPH0t3EkMX1N4qWwjomsjvoaQhOwFcfgqmPqJDcVrOuJCMufK38RUzfv7MRfglJDi1gmO3HHyKivZ3t12X72NymF01IOBExUgh83lAGeNDx6LHqZoRuYJlg.yR9YP39UT5uUbBXQmGCI4ixXqiA1lxFD5RHDuwIv1uU&dib_tag=se&keywords=deepseek+from+scratch&qid=1768319033&sprefix=deepseek+from+s%2Caps%2C106&sr=8-1)
* [Book - Demystifying Generative AI: A Practical and Intuitive Introduction ](https://www.amazon.com/Demystifying-Generative-Practical-Intuitive-Introduction/dp/0135429412/ref=sr_1_1?crid=3T6Y43XUPV2W6&dib=eyJ2IjoiMSJ9.HQe3QSze6HnFMP5XKUEvWlvACwl64U44fsPvayR4p7RZKYjVB2ZsJZnJJFkxtFBxhXFwEmBoFOC1tgMRB6WvFm1g3Ut3alJX4akIokHy77F-oggifKUlKzdhEk1XbKx-IYWQcXrk6BRXK6KZM3aIwVauL7yLCWrxD81LGAMkzebj50BD4w0I1Vxuywr0ImuQSUzDARPB1L0zoo6JOOcB2O3dh38v4WS__90LsP1LDBQ.o2Yk0s9iUxWzf5NhZddhBvTPmcUVdGm3YEOSjV4Kd9Y&dib_tag=se&keywords=demystifying+generative+ai&qid=1768588993&sprefix=demystifying+gen%2Caps%2C133&sr=8-1)

```unix
$ python --version
$ python3 --version
Python 3.11.13

$ brew install python@3.x.y
$ pip install uv
$ pip3.11 install uv

$ cd ~/Desktop
$ git clone https://github.com/rasbt/LLMs-from-scratch.git
$ cd ~/Desktop/LLMs-from-scratch

$ uv venv --python=python3.11
$ source .venv/bin/activate

$ which python
/Users/marshad/Desktop/LLMs-from-scratch/.venv/bin/python

$ python --version
Python 3.11.13

$ uv pip install packages

$ uv run jupyter lab
```

****

#### Quick Steps to Run NoteBook
```unix
$ cd ~/Desktop/LLMs-from-scratch
$ source .venv/bin/activate
$ uv run jupyter lab
```

***

#### Dr. Raj Dandekar
| Lecture | Notes | Date Updated |
|---|---|---|
|[GitHub Link for Book - Code](https://github.com/rasbt/LLMs-from-scratch) | [Code]((https://github.com/rasbt/LLMs-from-scratch)) | |
|---|---|---|
| [Lecture 01 -- Building LLMs from scratch: Series introduction](https://www.youtube.com/watch?v=Xpr8D6LeAtw) |  [Notes01](https://github.com/muarshad01/LLM/blob/main/Notes/lecture01_notes.md)| 01/01/2026 |
| [Lecture 02 -- LLM basics](https://www.youtube.com/watch?v=3dWzNZXA8DY)| [Notes02](https://github.com/muarshad01/LLM/blob/main/Notes/lecture02_notes.md) | 01/01/2026 |
| [Lecture 03 -- Pretraining LLMs vs finetuning LLMs](https://www.youtube.com/watch?v=-bsa3fCNGg4)|  [Notes03](https://github.com/muarshad01/LLM/blob/main/Notes/lecture03_notes.md) | 01/01/2026 |
| [Lecture 04 -- What are transformers?](https://www.youtube.com/watch?v=NLn4eetGmf8) |  [Notes04](https://github.com/muarshad01/LLM/blob/main/Notes/lecture04_notes.md) | 01/01/2026 |
| [Lecture 05 -- How does GPT-3 really work?](https://www.youtube.com/watch?v=xbaYCf2FHSY) |  [Notes05](https://github.com/muarshad01/LLM/blob/main/Notes/lecture05_notes.md) | 01/01/2026 |
| [Lecture 06 -- Stages of building an LLM from scratch](https://www.youtube.com/watch?v=z9fgKz1Drlc) |  [Notes06](https://github.com/muarshad01/LLM/blob/main/Notes/lecture06_notes.md) | 01/01/2026 |
| [Lecture 07 -- Code an LLM tokenizer from scratch in Python](https://www.youtube.com/watch?v=rsy5Ragmso8) |  [Notes07](https://github.com/muarshad01/LLM/blob/main/Notes/lecture07_notes.md) | 05/01/2026 |
| [Lecture 08 -- The GPT tokenizer: Byte Pair Encoding (BPE)](https://www.youtube.com/watch?v=fKd8s29e-l4) |  [Notes08](https://github.com/muarshad01/LLM/blob/main/Notes/lecture08_notes.md) | 05/01/2026 |
| [Lecture 09 -- Creating Input-Target data pairs using Python DataLoader](https://www.youtube.com/watch?v=iQZFH8dr2yI) |  [Notes09](https://github.com/muarshad01/LLM/blob/main/Notes/lecture09_notes.md) | 06/01/2026 |
| [Lecture 10 -- What are Token Embeddings?](https://www.youtube.com/watch?v=ghCSGRgVB_o) |  [Notes10](https://github.com/muarshad01/LLM/blob/main/Notes/lecture10_notes.md) | 07/01/2026 |
| [Lecture 11 -- The importance of Positional Embeddings](https://www.youtube.com/watch?v=ufrPLpKnapU) |  [Notes11](https://github.com/muarshad01/LLM/blob/main/Notes/lecture11_notes.md) | 08/01/2026 |
| [Lecture 12 -- The entire Data Preprocessing Pipeline of LLMs](https://www.youtube.com/watch?v=mk-6cFebjis) |  [Notes12](https://github.com/muarshad01/LLM/blob/main/Notes/lecture12_notes.md)| |
|---|---|
| [Lecture 13 -- Introduction to the Attention Mechanism in LLMs](https://www.youtube.com/watch?v=XN7sevVxyUM) |  [Notes13](https://github.com/muarshad01/LLM/blob/main/Notes/lecture13_notes.md) | 12/01/2026 |
| [Lecture 14 -- Simplified Attention Mechanism - Coded from scratch in Python - No trainable weights](https://www.youtube.com/watch?v=eSRhpYLerw4) |  [Notes14](https://github.com/muarshad01/LLM/blob/main/Notes/lecture14_notes.md) | 13/01/2026 |
| [Lecture 15 -- Coding the Self Attention Mechanism with key, query and value (K, Q, V) matrices](https://www.youtube.com/watch?v=UjdRN80c6p8) |  [Notes15](https://github.com/muarshad01/LLM/blob/main/Notes/lecture15_notes.md) | 14/01/2026 |
| [Lecture 16 -- Causal Self Attention Mechanism - Coded from scratch in Python](https://www.youtube.com/watch?v=h94TQOK7NRA) |  [Notes16](https://github.com/muarshad01/LLM/blob/main/Notes/lecture16_notes.md) | 15/01/2026 |
| [Lecture 17 -- Multi Head Attention Part 1 - Basics and Python code](https://www.youtube.com/watch?v=cPaBCoNdCtE) |  [Notes17](https://github.com/muarshad01/LLM/blob/main/Notes/lecture17_notes.md) | 15/01/2026 |
| [Lecture 18 -- Multi Head Attention Part 2 - Entire mathematics explained](https://www.youtube.com/watch?v=K5u9eEaoxFg) |  [Notes18](https://github.com/muarshad01/LLM/blob/main/Notes/lecture18_notes.md) | 15/01/2026 |
|---|---|
| [Lecture 19 -- Birds Eye View of the LLM Architecture](https://www.youtube.com/watch?v=4i23dYoXp-A) |  [Notes19](https://github.com/muarshad01/LLM/blob/main/Notes/lecture19_notes.md) | 16/01/2026 |
| [Lecture 20 -- Layer Normalization in the LLM Architecture](https://www.youtube.com/watch?v=G3W-LT79LSI) |  [Notes20](https://github.com/muarshad01/LLM/blob/main/Notes/lecture20_notes.md) ||
| [Lecture 21 --GELU Activation Function in the LLM Architecture](https://www.youtube.com/watch?v=d_PiwZe8UF4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21) |  [Notes21](https://github.com/muarshad01/LLM/blob/main/Notes/lecture21_notes.md) ||
| [Lecture 22 -- Shortcut connections in the LLM Architecture](https://www.youtube.com/watch?v=2r0QahNdwMw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=22) |  [Notes22](https://github.com/muarshad01/LLM/blob/main/Notes/lecture22_notes.md) ||
| [Lecture 23 -- Coding the entire LLM Transformer Block](https://www.youtube.com/watch?v=dvH6lFGhFrs&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=23) |  [Notes23](https://github.com/muarshad01/LLM/blob/main/Notes/lecture23_notes.md) ||
| [Lecture 24 -- Coding the 124 million parameter GPT-2 model](https://www.youtube.com/watch?v=G3-JgHckzjw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=24) |  [Notes24](https://github.com/muarshad01/LLM/blob/main/Notes/lecture24_notes.md) ||
| [Lecture 25 -- Coding GPT-2 to predict the next token](https://www.youtube.com/watch?v=F1Sm7z2R96w&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=25) |  [Notes25](https://github.com/muarshad01/LLM/blob/main/Notes/lecture25_notes.md) ||
| [Lecture 26 -- Measuring the LLM loss function](https://www.youtube.com/watch?v=7TKCrt--bWI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=26) |  [Notes26](https://github.com/muarshad01/LLM/blob/main/Notes/lecture26_notes.md) ||
| [Lecture 27 -- Evaluating LLM performance on real dataset - Hands on project - Book data](https://www.youtube.com/watch?v=zuj_NJNouAA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=27) |  [Notes27](https://github.com/muarshad01/LLM/blob/main/Notes/lecture27_notes.md) ||
| [Lecture 28 -- Coding the entire LLM Pre-training Loop](https://www.youtube.com/watch?v=Zxf-34voZss&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=28) |  [Notes28](https://github.com/muarshad01/LLM/blob/main/Notes/lecture28_notes.md) ||
| [Lecture 29 --Temperature Scaling in Large Language Models (LLMs)](https://www.youtube.com/watch?v=oG1FPVnY0pI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=29) |  [Notes29](https://github.com/muarshad01/LLM/blob/main/Notes/lecture29_notes.md) ||
| [Lecture 30 -- Top-k sampling in Large Language Models](https://www.youtube.com/watch?v=EhU32O7DkA4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=30) |  [Notes30](https://github.com/muarshad01/LLM/blob/main/Notes/lecture30_notes.md) ||
| [Lecture 31 -- Saving and loading LLM model weights using PyTorch](https://www.youtube.com/watch?v=Bc-9sf0VihQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=31) |  [Notes31](https://github.com/muarshad01/LLM/blob/main/Notes/lecture31_notes.md) ||
| [Lecture 32 -- Loading pre-trained weights from OpenAI GPT-2](https://www.youtube.com/watch?v=yXrGeDNuymY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=32) |  [Notes32](https://github.com/muarshad01/LLM/blob/main/Notes/lecture32_notes.md) ||
| [Lecture 33 -- Introduction to LLM Finetuning - Python Coding with hands-on-example](https://www.youtube.com/watch?v=yZpy_hsC1bE&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=33) |  [Notes33](https://github.com/muarshad01/LLM/blob/main/Notes/lecture33_notes.md) ||
| [Lecture 34 -- Dataloaders in LLM Classification Finetuning - Python Coding - Hands on LLM project](https://www.youtube.com/watch?v=f6zqClXOh7Y&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=34) |  [Notes34](https://github.com/muarshad01/LLM/blob/main/Notes/lecture34_notes.md) ||
| [Lecture 35 -- Coding the model architecture for LLM classification fine-tuning](https://www.youtube.com/watch?v=izyxvl-2JlM&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=35) |  [Notes35](https://github.com/muarshad01/LLM/blob/main/Notes/lecture35_notes.md) ||
| [Lecture 36 -- Coding a fine-tuned LLM spam classification model - From Scratch](https://www.youtube.com/watch?v=0PpxZ3kNPWo&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=36) |  [Notes36](https://github.com/muarshad01/LLM/blob/main/Notes/lecture36_notes.md) ||
| [Lecture 37 -- Introduction to LLM Instruction Fine-tuning - Loading Dataset - Alpaca Prompt format](https://www.youtube.com/watch?v=1bLhvqZzdaQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=37) |  [Notes37](https://github.com/muarshad01/LLM/blob/main/Notes/lecture37_notes.md) ||
| [Lecture 38 -- Data Batching in LLM instruction fine-tuning - Hands on project - Live Python coding](https://www.youtube.com/watch?v=bkUkcyL_Xxc&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=38) |  [Notes38](https://github.com/muarshad01/LLM/blob/main/Notes/lecture38_notes.md) ||
| [Lecture 39 -- Dataloaders in Instruction Fine-tuning](https://www.youtube.com/watch?v=egFqsJQ9kuY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=39) |  [Notes39](https://github.com/muarshad01/LLM/blob/main/Notes/lecture39_notes.md) ||
| [Lecture 40 -- Instruction fine-tuning: Loading pre-trained LLM weights](https://www.youtube.com/watch?v=__OiQznq4ao&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=40) |  [Notes40](https://github.com/muarshad01/LLM/blob/main/Notes/lecture40_notes.md) ||
| [Lecture 41 -- LLM fine-tuning training loop - Coded from scratch](https://www.youtube.com/watch?v=r7unILsP0Es&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=41) |  [Notes41](https://github.com/muarshad01/LLM/blob/main/Notes/lecture41_notes.md) ||
| [Lecture 42 -- Evaluating fine-tuned LLM using Ollama](https://www.youtube.com/watch?v=7m2jV7BOFkA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=42) |  [Notes42](https://github.com/muarshad01/LLM/blob/main/Notes/lecture42_notes.md) ||
| [Lecture 43 -- Build LLMs from scratch 20 minutes summary](https://www.youtube.com/watch?v=_xH-jXNFRjA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=43) |  [Notes43](https://github.com/muarshad01/LLM/blob/main/Notes/lecture43_notes.md) ||

***

## Deep Learning
|Lecture | Notes|
|---|---|
| [Deep Learning Chapter 1 -- But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) ||
| [Deep Learning Chapter 2 -- Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w) ||
| [Deep Learning Chapter 3 -- Backpropagation, intuitively](https://www.youtube.com/watch?v=Ilg3gGewQ5U) ||
| [Deep Learning Chapter 4 -- Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8) ||
| [Deep Learning Chapter 5 -- Transformers, the tech behind LLMs](https://www.youtube.com/watch?v=wjZofJX0v4M) ||
| [Deep Learning Chapter 6 -- Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc) ||
| [Deep Learning Chapter 7 -- How might LLMs store facts ](https://www.youtube.com/watch?v=9-Jl0dxWQs8) ||

***

* [Understanding Large Language Model - Under The Hood](https://www.youtube.com/playlist?list=PLUfbC589u-FSwnqsvTHXVcgmLg8UnbIy3)
* [How Attention Mechanism Works in Transformer Architecture](https://www.youtube.com/watch?v=KMHkbXzHn7s)
* [Generative Machine Learning - Attention Mechanisms with Math](https://www.youtube.com/playlist?list=PLs8w1Cdi-zvalz9ltXmarqyeQ49wfKFqf)
* [AGI Lambda](https://www.youtube.com/@AGI.Lambdaa/shorts)
* [Vision Transformers](https://www.youtube.com/shorts/qPUYBX0C6ic)
* [How RNNs Help AI Understand Language](https://www.youtube.com/shorts/w67EHFHGHUQ)
* [How does NN work in 60 seconds](https://www.youtube.com/shorts/Dbcx2_MO0LM)
* [BERT Networks in 60 seconds](https://www.youtube.com/shorts/HBOloY08auQ)
* [What is RAG](https://www.youtube.com/shorts/CbAQUqnrDcA)
* [MCP Protocol](https://www.youtube.com/shorts/7CHr0qwTcJw)
* [Build a Small Language Model (SLM) From Scratch](https://www.youtube.com/watch?v=pOFcwcwtv3k)
* [Large Language Models explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs&t=2s)
* [What is Tensor](https://www.youtube.com/shorts/J4Tg4gAPMMQ)
* [How word vectors encode meaning](https://www.youtube.com/shorts/FJtFZwbvkI4)
* [Self-Attention in Transformer](https://www.youtube.com/shorts/l8_OrR9kUNw)
* [I Visualised Attention in Transformers](https://www.youtube.com/watch?v=RNF0FvRjGZk)
* Autoencoders | Deep Learning Animated
* [The Most Important Algorithm in Machine Learning](https://www.youtube.com/watch?v=SmZmBKc7Lrs)
* [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)
* [ML Foundations for AI Engineers (in 34 Minutes)](https://www.youtube.com/watch?v=BUTjcAjfMgY)
* [How DeepSeek Rewrote the Transformer [MLA]](https://www.youtube.com/watch?v=0VLAoVGf_74)

***

#### [Open Source LLM Tools - Chip Huyen](https://huyenchip.com/llama-police.html)

***

#### [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://www.youtube.com/watch?v=IHZwWFHWa-w)

#### [What's The Difference Between Matrices And Tensors?](https://www.youtube.com/watch?v=1GwAEnegaRs)

#### [Visualizing transformers and attention | Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)

***

#### TODO
1. [Building LLMs from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)
2. [Build DeepSeek from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSiOpKKlHCyOq9lnp-dLvlms) 
3. [Reasoning LLMs from Scratch (Reinforcement Learning)](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSijcbUrRZHm6BrdinLuelPs)
5. [Neural Networks from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSj6tNyn_UadmUeU3Q3oR-hu)

***
