#### Books
* [Book Links](https://github.com/muarshad01/LLM/blob/main/books.md)

*** 

#### Installation
* [Installation on MacOS](https://github.com/muarshad01/LLM/blob/main/Installation.md)

***

#### Quick Steps to Run NoteBook
```unix
$ cd ~/Desktop/LLMs-from-scratch
$ source .venv/bin/activate
$ uv run jupyter lab
```

***

#### Building LLMs From Scratch
* [GitHub Link for Book - Code](https://github.com/rasbt/LLMs-from-scratch)
* [ALL Video Lectures](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu)
* [Vizura LLM Code](https://github.com/muarshad01/LLM/tree/main/vizuara-llm-code)

***

| Lecture | Notes | Date Updated |
|---|---|---|
| [Lecture 01 -- Building LLMs from scratch: Series introduction](https://www.youtube.com/watch?v=Xpr8D6LeAtw) |  [Notes01](https://github.com/muarshad01/LLM/blob/main/Notes/lecture01_notes.md)| Jan 01, 2026 |
| [Lecture 02 -- LLM basics](https://www.youtube.com/watch?v=3dWzNZXA8DY)| [Notes02](https://github.com/muarshad01/LLM/blob/main/Notes/lecture02_notes.md) | Jan 01, 2026 |
| [Lecture 03 -- Pretraining LLMs vs finetuning LLMs](https://www.youtube.com/watch?v=-bsa3fCNGg4)|  [Notes03](https://github.com/muarshad01/LLM/blob/main/Notes/lecture03_notes.md) | Jan 01, 2026 |
| [Lecture 04 -- What are transformers?](https://www.youtube.com/watch?v=NLn4eetGmf8) |  [Notes04](https://github.com/muarshad01/LLM/blob/main/Notes/lecture04_notes.md) | Jan 01, 2026 |
| [Lecture 05 -- How does GPT-3 really work?](https://www.youtube.com/watch?v=xbaYCf2FHSY) |  [Notes05](https://github.com/muarshad01/LLM/blob/main/Notes/lecture05_notes.md) | Jan 01, 2026 |
| [Lecture 06 -- Stages of building an LLM from scratch](https://www.youtube.com/watch?v=z9fgKz1Drlc) |  [Notes06](https://github.com/muarshad01/LLM/blob/main/Notes/lecture06_notes.md) | Jan 01, 2026 |
| [Lecture 07 -- Code an LLM tokenizer from scratch in Python](https://www.youtube.com/watch?v=rsy5Ragmso8) |  [Notes07](https://github.com/muarshad01/LLM/blob/main/Notes/lecture07_notes.md) | Jan 05, 2026 |
| [Lecture 08 -- The GPT tokenizer: Byte Pair Encoding (BPE)](https://www.youtube.com/watch?v=fKd8s29e-l4) |  [Notes08](https://github.com/muarshad01/LLM/blob/main/Notes/lecture08_notes.md) | Jan 05, 2026 |
| [Lecture 09 -- Creating Input-Target data pairs using Python DataLoader](https://www.youtube.com/watch?v=iQZFH8dr2yI) |  [Notes09](https://github.com/muarshad01/LLM/blob/main/Notes/lecture09_notes.md) | Jan 06, 2026 |
| [Lecture 10 -- What are Token Embeddings?](https://www.youtube.com/watch?v=ghCSGRgVB_o) |  [Notes10](https://github.com/muarshad01/LLM/blob/main/Notes/lecture10_notes.md) | Jan 07, 2026 |
| [Lecture 11 -- The importance of Positional Embeddings](https://www.youtube.com/watch?v=ufrPLpKnapU) |  [Notes11](https://github.com/muarshad01/LLM/blob/main/Notes/lecture11_notes.md) | Jan 08, 2026 |
| [Lecture 12 -- The entire Data Preprocessing Pipeline of LLMs](https://www.youtube.com/watch?v=mk-6cFebjis) |  [Notes12](https://github.com/muarshad01/LLM/blob/main/Notes/lecture12_notes.md)| |
|---|---|
| [Lecture 13 -- Introduction to the Attention Mechanism in LLMs](https://www.youtube.com/watch?v=XN7sevVxyUM) |  [Notes13](https://github.com/muarshad01/LLM/blob/main/Notes/lecture13_notes.md) | Jan 12, 2026 |
| [Lecture 14 -- Simplified Attention Mechanism - Coded from scratch in Python - No trainable weights](https://www.youtube.com/watch?v=eSRhpYLerw4) |  [Notes14](https://github.com/muarshad01/LLM/blob/main/Notes/lecture14_notes.md) | Jan 13, 2026 |
| [Lecture 15 -- Coding the Self Attention Mechanism with key, query and value (K, Q, V) matrices](https://www.youtube.com/watch?v=UjdRN80c6p8) |  [Notes15](https://github.com/muarshad01/LLM/blob/main/Notes/lecture15_notes.md) | Jan 14, 2026 |
| [Lecture 16 -- Causal Self Attention Mechanism - Coded from scratch in Python](https://www.youtube.com/watch?v=h94TQOK7NRA) |  [Notes16](https://github.com/muarshad01/LLM/blob/main/Notes/lecture16_notes.md) | Jan 15, 2026 |
| [Lecture 17 -- Multi-Head Attention (MHA) Part 1 - Basics and Python code](https://www.youtube.com/watch?v=cPaBCoNdCtE) |  [Notes17](https://github.com/muarshad01/LLM/blob/main/Notes/lecture17_notes.md) | Jan 15, 2026 |
| [Lecture 18 -- Multi-Head Attention (MHA) Part 2 - Entire mathematics explained](https://www.youtube.com/watch?v=K5u9eEaoxFg) |  [Notes18](https://github.com/muarshad01/LLM/blob/main/Notes/lecture18_notes.md) | Jan 15, 2026 |
|---|---|
| [Lecture 19 -- Birds Eye View of the LLM Architecture](https://www.youtube.com/watch?v=4i23dYoXp-A) |  [Notes19](https://github.com/muarshad01/LLM/blob/main/Notes/lecture19_notes.md) | Jan 16, 2026 |
| [Lecture 20 -- Layer Normalization in the LLM Architecture](https://www.youtube.com/watch?v=G3W-LT79LSI) |  [Notes20](https://github.com/muarshad01/LLM/blob/main/Notes/lecture20_notes.md) | Jan 17, 2026 |
| [Lecture 21 -- GELU Activation Function in the LLM Architecture](https://www.youtube.com/watch?v=d_PiwZe8UF4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=21) |  [Notes21](https://github.com/muarshad01/LLM/blob/main/Notes/lecture21_notes.md) | Jan 18, 2026 |
| [Lecture 22 -- Shortcut connections in the LLM Architecture](https://www.youtube.com/watch?v=2r0QahNdwMw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=22) |  [Notes22](https://github.com/muarshad01/LLM/blob/main/Notes/lecture22_notes.md) | Jan 18, 2026 |
| [Lecture 23 -- Coding the entire LLM Transformer Block](https://www.youtube.com/watch?v=dvH6lFGhFrs&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=23) |  [Notes23](https://github.com/muarshad01/LLM/blob/main/Notes/lecture23_notes.md) | Jan 19, 2026 |
| [Lecture 24 -- Coding the 124 million parameter GPT-2 model](https://www.youtube.com/watch?v=G3-JgHckzjw&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=24) |  [Notes24](https://github.com/muarshad01/LLM/blob/main/Notes/lecture24_notes.md) | Jan 20, 2026 |
| [Lecture 25 -- Coding GPT-2 to predict the next token](https://www.youtube.com/watch?v=F1Sm7z2R96w&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=25) |  [Notes25](https://github.com/muarshad01/LLM/blob/main/Notes/lecture25_notes.md) | Jan 21, 2026 |
|---|---|---|
| [Lecture 26 -- Measuring the LLM loss function](https://www.youtube.com/watch?v=7TKCrt--bWI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=26) |  [Notes26](https://github.com/muarshad01/LLM/blob/main/Notes/lecture26_notes.md) | Jan 22, 2026 |
| [Lecture 27 -- Evaluating LLM performance on real dataset - Hands on project - Book data](https://www.youtube.com/watch?v=zuj_NJNouAA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=27) |  [Notes27](https://github.com/muarshad01/LLM/blob/main/Notes/lecture27_notes.md) | Jan 23, 2026 |
|---|---|---|
| [Lecture 28 -- Coding the entire LLM Pre-training Loop](https://www.youtube.com/watch?v=Zxf-34voZss&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=28) |  [Notes28](https://github.com/muarshad01/LLM/blob/main/Notes/lecture28_notes.md) | Jan 24, 2026 |
| [Lecture 29 --Temperature Scaling in LLMs](https://www.youtube.com/watch?v=oG1FPVnY0pI&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=29) |  [Notes29](https://github.com/muarshad01/LLM/blob/main/Notes/lecture29_notes.md) | Jan 25, 2026 |
| [Lecture 30 -- Top-k Sampling in LLMs](https://www.youtube.com/watch?v=EhU32O7DkA4&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=30) |  [Notes30](https://github.com/muarshad01/LLM/blob/main/Notes/lecture30_notes.md) | Jan 25, 2026 |
|---|---|---|
| [Lecture 31 -- Saving and loading LLM model weights using PyTorch](https://www.youtube.com/watch?v=Bc-9sf0VihQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=31) |  [Notes31](https://github.com/muarshad01/LLM/blob/main/Notes/lecture31_notes.md) | Jan 25, 2026 |
| [Lecture 32 -- Loading pre-trained weights from OpenAI GPT-2](https://www.youtube.com/watch?v=yXrGeDNuymY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=32) |  [Notes32](https://github.com/muarshad01/LLM/blob/main/Notes/lecture32_notes.md) | Jan 26, 2026 |
|---|---|---|
| [Lecture 33 -- Introduction to LLM Finetuning - Python Coding with hands-on-example](https://www.youtube.com/watch?v=yZpy_hsC1bE&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=33) |  [Notes33](https://github.com/muarshad01/LLM/blob/main/Notes/lecture33_notes.md) | Jan 26, 2026 |
| [Lecture 34 -- Dataloaders in LLM Classification Finetuning - Python Coding - Hands on LLM project](https://www.youtube.com/watch?v=f6zqClXOh7Y&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=34) |  [Notes34](https://github.com/muarshad01/LLM/blob/main/Notes/lecture34_notes.md) | Jan 27, 2026 |
| [Lecture 35 -- Coding the model architecture for LLM classification fine-tuning](https://www.youtube.com/watch?v=izyxvl-2JlM&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=35) |  [Notes35](https://github.com/muarshad01/LLM/blob/main/Notes/lecture35_notes.md) | Jan 29, 2026 |
| [Lecture 36 -- Coding a fine-tuned LLM spam classification model - From Scratch](https://www.youtube.com/watch?v=0PpxZ3kNPWo&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=36) |  [Notes36](https://github.com/muarshad01/LLM/blob/main/Notes/lecture36_notes.md) | Jan 31, 2026 |
|---|---|---|
| [Lecture 37 -- Introduction to LLM Instruction Fine-tuning - Loading Dataset - Alpaca Prompt format](https://www.youtube.com/watch?v=1bLhvqZzdaQ&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=37) |  [Notes37](https://github.com/muarshad01/LLM/blob/main/Notes/lecture37_notes.md) | Jan 26, 2026 |
| [Lecture 38 -- Data Batching in LLM instruction fine-tuning - Hands on project - Live Python coding](https://www.youtube.com/watch?v=bkUkcyL_Xxc&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=38) |  [Notes38](https://github.com/muarshad01/LLM/blob/main/Notes/lecture38_notes.md) | Feb 02,2026|
| [Lecture 39 -- Dataloaders in Instruction Fine-tuning](https://www.youtube.com/watch?v=egFqsJQ9kuY&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=39) |  [Notes39](https://github.com/muarshad01/LLM/blob/main/Notes/lecture39_notes.md) | Feb 03, 2026 |
| [Lecture 40 -- Instruction fine-tuning: Loading pre-trained LLM weights](https://www.youtube.com/watch?v=__OiQznq4ao&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=40) |  [Notes40](https://github.com/muarshad01/LLM/blob/main/Notes/lecture40_notes.md) | Feb 04, 2026 |
| [Lecture 41 -- LLM fine-tuning training loop - Coded from scratch](https://www.youtube.com/watch?v=r7unILsP0Es&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=41) |  [Notes41](https://github.com/muarshad01/LLM/blob/main/Notes/lecture41_notes.md) | Feb 05, 2026 |
| [Lecture 42 -- Evaluating fine-tuned LLM using Ollama](https://www.youtube.com/watch?v=7m2jV7BOFkA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=42) |  [Notes42](https://github.com/muarshad01/LLM/blob/main/Notes/lecture42_notes.md) | Feb 07,2026 |
| [Lecture 43 -- Build LLMs from scratch 20 minutes summary](https://www.youtube.com/watch?v=_xH-jXNFRjA&list=PLPTV0NXA_ZSgsLAr8YCgCwhPIJNNtexWu&index=43) |  [Notes43](https://github.com/muarshad01/LLM/blob/main/Notes/lecture43_notes.md) | Feb 07, 2026|

***

## Deep Learning
|Lecture | Notes| Date |
|---|---|---|
| [Deep Learning Chapter 1 -- But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) |||
| [Deep Learning Chapter 2 -- Gradient descent, how neural networks learn](https://www.youtube.com/watch?v=IHZwWFHWa-w) |||
| [Deep Learning Chapter 3 -- Backpropagation, intuitively](https://www.youtube.com/watch?v=Ilg3gGewQ5U) |||
| [Deep Learning Chapter 4 -- Backpropagation calculus](https://www.youtube.com/watch?v=tIeHLnjs5U8) |||
| [Deep Learning Chapter 5 -- Transformers, the tech behind LLMs](https://www.youtube.com/watch?v=wjZofJX0v4M) |||
| [Deep Learning Chapter 6 -- Attention in transformers, step-by-step](https://www.youtube.com/watch?v=eMlx5fFNoYc) |||
| [Deep Learning Chapter 7 -- How might LLMs store facts ](https://www.youtube.com/watch?v=9-Jl0dxWQs8) |||

***


| Lecture	Notes	| Date| 
|---|---|
| [How word vectors encode meaning](https://www.youtube.com/shorts/FJtFZwbvkI4) | Feb 08, 2026 |
| [What is Tensor](https://www.youtube.com/shorts/J4Tg4gAPMMQ) | Feb 08, 2026 |
| [What's The Difference Between Matrices And Tensors?](https://www.youtube.com/watch?v=1GwAEnegaRs) ||
|---|---|
| [LLM Architecture Explained in 60 seconds](https://www.youtube.com/watch?v=xBKVlsnA6z4&list=PLPTV0NXA_ZSiXq_kpauJ_G5kejrFsM_yF) ||
| [Understanding Large Language Model (LLM) - Under The Hood](https://www.youtube.com/playlist?list=PLUfbC589u-FSwnqsvTHXVcgmLg8UnbIy3)||
| [Large Language Models (LLMs) explained briefly](https://www.youtube.com/watch?v=LPZh9BOjkQs&t=2s)||
| [Build a Small Language Model (SLM) From Scratch](https://www.youtube.com/watch?v=pOFcwcwtv3k)||
|---|---|
| [How Attention Mechanism Works in Transformer Architecture](https://www.youtube.com/watch?v=KMHkbXzHn7s)||
| [I Visualised Attention in Transformers](https://www.youtube.com/watch?v=RNF0FvRjGZk)||
| [Visualizing transformers and attention - Talk for TNG Big Tech Day '24](https://www.youtube.com/watch?v=KJtZARuO3JY)||
| [Self-Attention in Transformer](https://www.youtube.com/shorts/l8_OrR9kUNw)||
| [Generative Machine Learning - Attention Mechanisms with Math](https://www.youtube.com/playlist?list=PLs8w1Cdi-zvalz9ltXmarqyeQ49wfKFqf)||
| [Vision Transformers](https://www.youtube.com/shorts/qPUYBX0C6ic)||
| [How DeepSeek Rewrote the Transformer [MLA]](https://www.youtube.com/watch?v=0VLAoVGf_74)||
|---|---|
| [How RNNs Help AI Understand Language](https://www.youtube.com/shorts/w67EHFHGHUQ)||
| [How does NN work in 60 seconds](https://www.youtube.com/shorts/Dbcx2_MO0LM) | Feb 21, 2026 |
| [BERT Networks in 60 seconds](https://www.youtube.com/shorts/HBOloY08auQ) | Feb 19, 2026 |
| [What is RAG](https://www.youtube.com/shorts/CbAQUqnrDcA) | Feb 23, 2026 |
| [MCP Protocol](https://www.youtube.com/shorts/7CHr0qwTcJw) | Feb 21, 2026 |
| [AGI Lambda](https://www.youtube.com/@AGI.Lambdaa/shorts)||
|---|---|
| [Deep Learning 101 - Cross-Entropy Loss Function Demystified](https://www.youtube.com/watch?v=FODwUM-1PyI)||
| [A Review of 10 Most Popular Activation Functions in Neural Networks](https://www.youtube.com/watch?v=56ZxEmGRt2k)||
|---|---|
| [Autoencoders - Deep Learning Animated](https://www.youtube.com/watch?v=hZ4a4NgM3u0)||
| [The Most Important Algorithm in Machine Learning](https://www.youtube.com/watch?v=SmZmBKc7Lrs)||
| [ML Foundations for AI Engineers (in 34 Minutes)](https://www.youtube.com/watch?v=BUTjcAjfMgY)||

***

#### [Open Source LLM Tools - Chip Huyen](https://huyenchip.com/llama-police.html)

***

#### TODO
1. [Reasoning LLMs from Scratch (Reinforcement Learning)](https://www.youtube.com/playlist?list=PLPTV0NXA_ZSijcbUrRZHm6BrdinLuelPs)
2. [Build a Diffusion Language Model from Scratch](https://www.youtube.com/playlist?list=PLPTV0NXA_ZShhDDPgy1ygii42nwOngUaf)

***

#### AI/ML Companies
* [ai71 - UAE](https://ai71.ai/careers)
* [HUMAIN - Saudi Arabia](https://www.humain.com/)
  * Phase-1: 25,000 NVIDIA H100 GPUS (world's most advanced)
  * [HUMAIN and NVIDIA Announce Strategic Partnership to Build AI Factories of the Future in Saudi Arabia](https://nvidianews.nvidia.com/news/humain-and-nvidia-announce-strategic-partnership-to-build-ai-factories-of-the-future-in-saudi-arabia) 
* [QAI - Qatar](https://www.qailab.qa/)
* [Qater Investment Authority (QIA)](https://www.qia.qa/en/pages/default.aspx)
* Qai (Qatar Artificial Intelligence Company)

***
